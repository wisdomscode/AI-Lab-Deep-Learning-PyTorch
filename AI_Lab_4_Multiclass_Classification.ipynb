{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXTEPnaDaYQPletP7HP6eO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wisdomscode/AI-Lab-Deep-Learning-PyTorch/blob/main/AI_Lab_4_Multiclass_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiclass Classification"
      ],
      "metadata": {
        "id": "2S3SaNdLjuOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:** In this lesson, we'll work with the full wildlife dataset, which has eight classes. This is more than the network in the previous notebook can handle. Here we'll build and train a more complicated neural network, called a Convolutional Neural Network, that is meant for working with images. We'll use this network to get the predictions we need for the competition at [DrivenData.org](https://www.drivendata.org/competitions/87/competition-image-classification-wildlife-conservation/)."
      ],
      "metadata": {
        "id": "jJeltkzakK95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objectives:**\n",
        "\n",
        "* Read in data with multiple classes\n",
        "* Normalize our data to improve performance\n",
        "* Create a Convolutional Neural Network that works well with images\n",
        "* Train that network to do multiclass classification\n",
        "* Reformat the network predictions to complete the competition\n",
        "\n",
        "**New Terms:**\n",
        "\n",
        "* Multiclass\n",
        "* Normalize\n",
        "* Convolution\n",
        "* Max pooling"
      ],
      "metadata": {
        "id": "u-1n7OaqkcIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Competition"
      ],
      "metadata": {
        "id": "soFaML3Gk3M-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a reminder, the data we're working with came from a competition at [DrivenData.org](https://www.drivendata.org/competitions/87/competition-image-classification-wildlife-conservation/). The goal of the competition is to build a model that takes an image and classifies what animal is in it. There are seven animals, plus a 'blank' where no animal is present in the image.\n",
        "\n",
        "So far, we have\n",
        "\n",
        "* Read in image data\n",
        "* Loaded that data in PyTorch\n",
        "* Built a neural network\n",
        "* Used that neural network to do binary classification\n",
        "\n",
        "We're almost ready for the competition now. We need to expand our network to handle all eight categories. We could do this with the simple network we've already built, but it would perform poorly. Instead, we're going to build a more complex network that's meant for working with images. This is called a Convolutional Neural Network, and involves arranging the neurons in a different pattern.\n",
        "\n",
        "Once we have built and trained this network, its output will be what the competition is looking for. We'll get our predictions and save them into the requested format."
      ],
      "metadata": {
        "id": "75g0-AYjlUdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started"
      ],
      "metadata": {
        "id": "Cse_xpSumPKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we'll import the packages we'll need in this notebook."
      ],
      "metadata": {
        "id": "7yf3ny7TmeXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cUe_LgMnjfmN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchinfo import summary\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdNmkKI1pqc0",
        "outputId": "e07991d2-5717-482c-d9d7-a677394220e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print out the versions of our packages again. If we come back to this later, we'll know what we used."
      ],
      "metadata": {
        "id": "SlvrO6wvmxmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Platform:\", sys.platform)\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"---\")\n",
        "print(\"matplotlib version:\", matplotlib.__version__)\n",
        "print(\"pandas version:\", pd.__version__)\n",
        "print(\"PIL version:\", PIL.__version__)\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"torchvision version:\", torchvision.__version__)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf7eGY8WmvkB",
        "outputId": "e893268c-65c9-4ae2-9107-f692cc0d4519"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Platform: linux\n",
            "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "---\n",
            "matplotlib version: 3.10.0\n",
            "pandas version: 2.2.2\n",
            "PIL version: 11.1.0\n",
            "torch version: 2.5.1+cu124\n",
            "torchvision version: 0.20.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should be running on GPUs, so the device should be `cuda`."
      ],
      "metadata": {
        "id": "RAoZwpr4pQsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using {device} device.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qyi1BN7p8_-",
        "outputId": "be572542-efa0-45a1-c316-d0705df32af2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading files"
      ],
      "metadata": {
        "id": "V5PAZxlwqDbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need to read in our data. Since we'll be using images once again, we'll need to convert them to something our network can understand. To start with, we'll use the same set of transformations we used in the previous notebook.\n",
        "\n",
        "These transformations are\n",
        "\n",
        "* Convert any grayscale images to RGB format with a custom class\n",
        "* Resize the image, so that they're all the same size (we chose\n",
        "224 x 224, but other sizes would work as well)\n",
        "* Convert the image to a Tensor of pixel values\n",
        "\n",
        "This should result in each image becoming a Tensor of size\n",
        "3 x 224 x 224. We'll check this once we read in the data."
      ],
      "metadata": {
        "id": "TGWaRwuMqXR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvertToRGB:\n",
        "    def __call__(self, img):\n",
        "        if img.mode != \"RGB\":\n",
        "            img = img.convert(\"RGB\")\n",
        "        return img"
      ],
      "metadata": {
        "id": "puvU6YRpq1cw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        ConvertToRGB(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "BBAEZ6TEq69T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous notebook, we were working with only two categories. That data was in the data_binary subdirectory. Here we'll work with all eight categories, in the data_multiclass subdirectory. Let's load that data. We will follow the same pattern we used in the last notebook."
      ],
      "metadata": {
        "id": "pAdUFZVKrLnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.4.1:** Assign the path to the multi-class training data to train_dir. Then use the ImageFolder tool to open those files and apply our transforms."
      ],
      "metadata": {
        "id": "IB6yk3fIrPC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'data_p1/data_multiclass/'\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "\n",
        "print(\"Will read data from\", train_dir)\n",
        "\n",
        "#output\n",
        "Will read data from data_p1/data_multiclass/train"
      ],
      "metadata": {
        "id": "j_pF5RvgsxUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root=train_dir, transform=transform)"
      ],
      "metadata": {
        "id": "flmM7deHtAzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our data, let's verify that we got what we wanted. We should have classes for each of the seven animals, and one 'blank' for when there wasn't an animal in the image. Additionally, the tensors we get should be of size 3 x 224 x 224."
      ],
      "metadata": {
        "id": "u3rxVHJktJyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classes:\")\n",
        "print(dataset.classes)\n",
        "print(f\"That's {len(dataset.classes)} classes\")\n",
        "print()\n",
        "print(\"Tensor shape for one image:\")\n",
        "print(dataset[0][0].shape)\n",
        "\n",
        "#output\n",
        "Classes:\n",
        "['antelope_duiker', 'bird', 'blank', 'civet_genet', 'hog', 'leopard', 'monkey_prosimian', 'rodent']\n",
        "That's 8 classes\n",
        "\n",
        "Tensor shape for one image:\n",
        "torch.Size([3, 224, 224])"
      ],
      "metadata": {
        "id": "iGh5Q3fztVQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In principle, we could work with the data like this. But PyTorch is expecting the data to be broken into batches with a DataLoader. This prevents PyTorch from trying to load all of the files into memory at once, which would cause our notebook to crash. Instead, it loads just a few (the batch_size), works with them, then discards them. Since all the tools are expecting it, we should convert ours. The batch size to work with will depend on our system, but something in the\n",
        "20 to 100 range is usually fine. We'll pick 32."
      ],
      "metadata": {
        "id": "DQm_N8W3tSVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "dataset_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "# Get one batch\n",
        "first_batch = next(iter(dataset_loader))\n",
        "\n",
        "print(f\"Shape of one batch: {first_batch[0].shape}\")\n",
        "print(f\"Shape of labels: {first_batch[1].shape}\")\n",
        "\n",
        "#output\n",
        "Shape of one batch: torch.Size([32, 3, 224, 224])\n",
        "Shape of labels: torch.Size([32])"
      ],
      "metadata": {
        "id": "0KeIoJ6Zuesx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we loop over this loader, it'll produce small batches of our images. This is what we want â€” these are the \"minibatches\" that will speed up our computations. In our case, each batch is\n",
        "32 images, with each image\n",
        "3 x 224 x 224\n",
        ". It also provides us with the labels for the correct answers. This is the information we need to train a network."
      ],
      "metadata": {
        "id": "9K9Vbo11tUg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ljfu_BljuX2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Our Data"
      ],
      "metadata": {
        "id": "-JIlDj8-wnm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we were reading in the data, we already did some preparation. Our images are all the same shape, and have been converted to tensors. But neural networks tend to perform best with data that has a mean of 0 and a standard deviation of 1. Data that has that property is called *normalized*. In our case, that would be the mean and standard deviation of all of the pixels in all of the images.\n",
        "\n",
        "Let's see what they are for our data. Here's a function that computes the mean and standard deviation for each color channel (red, green, and blue) separately. It takes in a DataLoader and returns the mean and standard deviation of each channel."
      ],
      "metadata": {
        "id": "7BnYN6Hgw628"
      }
    }
  ]
}