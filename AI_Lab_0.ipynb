{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wisdomscode/AI-Lab-Deep-Learning-PyTorch/blob/main/AI_Lab_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Tensor"
      ],
      "metadata": {
        "id": "A7FQHMu9RC-J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r3Ii8znuJZV6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-tA5crqonRI",
        "outputId": "62d8cba2-3eba-437e-88f1-9d4de98f58e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Platform: linux\n",
            "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "---\n",
            "matplotlib version: 3.10.0\n",
            "pandas version: 2.2.2\n",
            "PIL version: 11.1.0\n",
            "torch version: 2.5.1+cu124\n",
            "torchvision version: 0.20.1+cu124\n"
          ]
        }
      ],
      "source": [
        "print(\"Platform:\", sys.platform)\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"---\")\n",
        "print(\"matplotlib version:\", matplotlib.__version__)\n",
        "print(\"pandas version:\", pd.__version__)\n",
        "print(\"PIL version:\", PIL.__version__)\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"torchvision version:\", torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXgd0gQ0pSBv",
        "outputId": "f06c71e5-3b6d-49ad-a15d-0de03fe5cb55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_tensor class: <class 'torch.Tensor'>\n",
            "tensor([[ 1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.],\n",
            "        [ 7.,  8.,  9.],\n",
            "        [10., 11., 12.]])\n"
          ]
        }
      ],
      "source": [
        "# Declare my_tensor\n",
        "my_values = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n",
        "my_tensor = torch.Tensor(my_values)\n",
        "\n",
        "print(\"my_tensor class:\", type(my_tensor))\n",
        "print(my_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor Attributes"
      ],
      "metadata": {
        "id": "7hL4dbXITLAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.2"
      ],
      "metadata": {
        "id": "WZBZbZwrVEAJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EZzJ6THpirp",
        "outputId": "289c6b6d-1e0c-4135-e9ea-a5d64d5135cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my_tensor shape: torch.Size([4, 3])\n",
            "my_tensor dtype: torch.float32\n"
          ]
        }
      ],
      "source": [
        "#Tensor attributes eg (dimenstions 'shape' and data type 'dtype')\n",
        "print(\"my_tensor shape:\", my_tensor.shape)\n",
        "\n",
        "print(\"my_tensor dtype:\", my_tensor.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.3"
      ],
      "metadata": {
        "id": "rLxNOiKYVJqp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A7A29JRqayu",
        "outputId": "4f5e88aa-fd25-4631-c7b8-6de7555c3747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my_tensor device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Tensor Device - Tell us the hardware tensor is installed\n",
        "print(\"my_tensor device:\", my_tensor.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlZSU2S8rHI6",
        "outputId": "33977a1f-f0d0-4595-b5d7-6aedbe304a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda GPUs available: True\n"
          ]
        }
      ],
      "source": [
        "#Some computers come with GPUs, which allow for bigger and faster model building.\n",
        "#In PyTorch, the cuda package is used to access GPUs on Linux and Windows machines;\n",
        "#mps is used on Macs.\n",
        "\n",
        "# Check if GPUs available via `cuda`\n",
        "cuda_gpus_available = torch.cuda.is_available()\n",
        "\n",
        "print(\"cuda GPUs available:\", cuda_gpus_available)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.4 Change Device"
      ],
      "metadata": {
        "id": "OnVFI4dmV_MZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSi86kPSsc4l",
        "outputId": "176c1586-1f78-422c-b91e-d909aa89cacb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_tensor device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Change the device from cpu to gpu 'cuda'\n",
        "my_tensor = my_tensor.to(\"cuda\")\n",
        "\n",
        "print(\"my_tensor device:\", my_tensor.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor Slicing"
      ],
      "metadata": {
        "id": "_BY21FJ5Yb-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.5"
      ],
      "metadata": {
        "id": "gDwH447GYkTq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pZDTMI8uSse",
        "outputId": "d7fd97ca-21dd-4bba-d34b-1a56459848ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "left_tensor class: <class 'torch.Tensor'>\n",
            "left_tensor shape: torch.Size([2, 3])\n",
            "left_tensor data type: torch.float32\n",
            "left_tensor device: cuda:0\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]], device='cuda:0')\n",
            "\n",
            "right_tensor class: <class 'torch.Tensor'>\n",
            "right_tensor shape: torch.Size([2, 3])\n",
            "right_tensor data type: torch.float32\n",
            "right_tensor device: cuda:0\n",
            "tensor([[ 7.,  8.,  9.],\n",
            "        [10., 11., 12.]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#Tensor Slicing - using [] and indexing to select a subset of the values in a tensor\n",
        "#Slice my_tensor, assigning its top two rows to left_tensor and its bottom two rows to right_tensor.\n",
        "left_tensor = my_tensor[:2 :]\n",
        "right_tensor = my_tensor[2: :]\n",
        "\n",
        "print(\"left_tensor class:\", type(left_tensor))\n",
        "print(\"left_tensor shape:\", left_tensor.shape)\n",
        "print(\"left_tensor data type:\", left_tensor.dtype)\n",
        "print(\"left_tensor device:\", left_tensor.device)\n",
        "print(left_tensor)\n",
        "print()\n",
        "print(\"right_tensor class:\", type(right_tensor))\n",
        "print(\"right_tensor shape:\", right_tensor.shape)\n",
        "print(\"right_tensor data type:\", right_tensor.dtype)\n",
        "print(\"right_tensor device:\", right_tensor.device)\n",
        "print(right_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor Math"
      ],
      "metadata": {
        "id": "TJz80WKwZE-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.6:  Addition"
      ],
      "metadata": {
        "id": "5qOEEuvyZH4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yDKVK14yvXNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2bc303-c316-480f-d324-8a0ad53ff079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 8., 10., 12.],\n",
            "        [14., 16., 18.]], device='cuda:0')\n",
            "\n",
            "tensor([[ 8., 10., 12.],\n",
            "        [14., 16., 18.]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#Assign the results to summed_tensor_operator and summed_tensor_method (+ or add())\n",
        "\n",
        "summed_tensor_operator = left_tensor + right_tensor\n",
        "summed_tensor_method = left_tensor.add(right_tensor)\n",
        "\n",
        "print(summed_tensor_operator)\n",
        "print()\n",
        "\n",
        "print(summed_tensor_method)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.7: Multiplication"
      ],
      "metadata": {
        "id": "vWq4rK-QaG2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# element-wise multiplication, where the corresponding values of two tensors are multiplied together.\n",
        "# In PyTorch, we can do this using the * operator or the .mul() method\n",
        "# To multiply number of rows of each must be equal\n",
        "# [[2, 5], [7, 3], [3]] and [[8], [9], [2], [5]]\n",
        "\n",
        "ew_tensor_operator = left_tensor * right_tensor\n",
        "ew_tensor_method = left_tensor.mul(right_tensor)\n",
        "\n",
        "\n",
        "print(ew_tensor_operator)\n",
        "print()\n",
        "\n",
        "print(ew_tensor_method)\n",
        "\n",
        "# Note that element-wise multiplication is commutative.\n",
        "# It doesn't matter in what order we multiply the two tensors.\n",
        "# The product of left_tensor * right_tensor is the same as the product of right_tensor * left_tensor.\n",
        "# left_tensor * right_tensor == right_tensor * left_tensor\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBRLuUjAaRs5",
        "outputId": "b463a5c3-5de4-4ccd-9f14-739f98cd16fb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 7., 16., 27.],\n",
            "        [40., 55., 72.]], device='cuda:0')\n",
            "\n",
            "tensor([[ 7., 16., 27.],\n",
            "        [40., 55., 72.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experimenting\n",
        "a = torch.Tensor([[2, 5, 1], [7, 3, 2], [4, 5, 3]])\n",
        "b = torch.Tensor([[8], [9], [5]])\n",
        "\n",
        "print(a * b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWnjP9VagyIL",
        "outputId": "ac2e3ac2-9cf7-41f1-ab05-87b81b93ac42"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[16., 40.,  8.],\n",
            "        [63., 27., 18.],\n",
            "        [20., 25., 15.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrix multiplication"
      ],
      "metadata": {
        "id": "njLpO3tscS-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrix multiplication, which combines the rows and columns of two tensors to generate a new one. We can use the @ operator or the .matmul() method."
      ],
      "metadata": {
        "id": "WjoesbpPc9ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets create two new matrices\n",
        "\n",
        "new_left_tensor = torch.Tensor([[2, 5], [7, 3]])\n",
        "new_right_tensor = torch.Tensor([[8], [9]])\n",
        "\n",
        "print(new_left_tensor)\n",
        "print()\n",
        "print(new_right_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stWgNxScdVVK",
        "outputId": "b699c7b2-45d3-4cb9-f17a-7e08908a23c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 5.],\n",
            "        [7., 3.]])\n",
            "\n",
            "tensor([[8.],\n",
            "        [9.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.8"
      ],
      "metadata": {
        "id": "8CYoYsftdXIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mm_tensor_operator = new_left_tensor @ new_right_tensor\n",
        "mm_tensor_method = new_left_tensor.matmul(new_right_tensor)\n",
        "\n",
        "print(mm_tensor_operator)\n",
        "print()\n",
        "print(mm_tensor_method)\n",
        "\n",
        "# Matrix multiplication is not commutative.\n",
        "# The number of columns in the tensor on the left must equal\n",
        "# the number of rows in the tensor on the right.\n",
        "# If these two dimensions don't match, your code will throw a RunTimeError."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvP54HBTc1xi",
        "outputId": "272a058b-9960-4510-b872-5a1f88cd15f6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[61.],\n",
            "        [83.]])\n",
            "\n",
            "tensor([[61.],\n",
            "        [83.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.9: Calculate the mean"
      ],
      "metadata": {
        "id": "rGL52OeviW8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor_mean = my_tensor.mean()\n",
        "\n",
        "print(\"my_tensor_mean class:\", type(my_tensor_mean))\n",
        "print(\"my_tensor_mean shape:\", my_tensor_mean.shape)\n",
        "print(\"my_tensor_mean data type:\", my_tensor_mean.dtype)\n",
        "print(\"my_tensor_mean device:\", my_tensor_mean.device)\n",
        "print()\n",
        "print(\"my_tensor mean:\", my_tensor_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QaL1uMpc_p7",
        "outputId": "1b4f339f-8d36-4627-9b1b-f2f1599f3c64"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_tensor_mean class: <class 'torch.Tensor'>\n",
            "my_tensor_mean shape: torch.Size([])\n",
            "my_tensor_mean data type: torch.float32\n",
            "my_tensor_mean device: cuda:0\n",
            "\n",
            "my_tensor mean: tensor(6.5000, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.10: Calculate the mean for each column in my_tensor."
      ],
      "metadata": {
        "id": "qcGvF218i43z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor_column_means = my_tensor.mean(dim=0)\n",
        "my_tensor_row_means = my_tensor.mean(dim=1)\n",
        "\n",
        "print(\"my_tensor:\", my_tensor)\n",
        "print(\"my_tensor column means:\", my_tensor_column_means)\n",
        "print(\"my_tensor row means:\", my_tensor_row_means)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCvBIr2Wi6cb",
        "outputId": "62c2d2f1-2083-44c2-e5a5-31dd92ffe8b6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_tensor: tensor([[ 1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.],\n",
            "        [ 7.,  8.,  9.],\n",
            "        [10., 11., 12.]], device='cuda:0')\n",
            "my_tensor column means: tensor([5.5000, 6.5000, 7.5000], device='cuda:0')\n",
            "my_tensor row means: tensor([ 2.,  5.,  8., 11.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore Files"
      ],
      "metadata": {
        "id": "VCyX1Dhfj6YV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.11"
      ],
      "metadata": {
        "id": "_pMPHWUukHt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the pattern of data_dir, assign the path to the multi-class training data to train_dir"
      ],
      "metadata": {
        "id": "9pbMIhmQkKak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to connect to your data files\n",
        "data_dir = os.path.join(\"data_p1\", \"data_multiclass\") # locate the main folder\n",
        "train_dir = os.path.join(data_dir, \"train\") # locate sub_folder\n",
        "\n",
        "print(\"data_dir class:\", type(data_dir))\n",
        "print(\"Data directory:\", data_dir)\n",
        "print()\n",
        "print(\"train_dir class:\", type(train_dir))\n",
        "print(\"Training data directory:\", train_dir)\n",
        "\n",
        "# output\n",
        "data_dir class: <class 'str'>\n",
        "Data directory: data_p1/data_multiclass\n",
        "\n",
        "train_dir class: <class 'str'>\n",
        "Training data directory: data_p1/data_multiclass/train"
      ],
      "metadata": {
        "id": "LPpts28GkNgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.12"
      ],
      "metadata": {
        "id": "ts6scnTXoGCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a list of the contents of train_dir, and assign the result to class_directories."
      ],
      "metadata": {
        "id": "s3VgYO1FoI9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_directories = os.listdir(train_dir)\n",
        "\n",
        "print(\"class_directories type:\", type(class_directories))\n",
        "print(\"class_directories length:\", len(class_directories))\n",
        "print(class_directories)\n",
        "\n",
        "# output\n",
        "class_directories type: <class 'list'>\n",
        "class_directories length: 8\n",
        "['hog', 'blank', 'monkey_prosimian', 'antelope_duiker', 'leopard', 'civet_genet', 'bird', 'rodent']\n"
      ],
      "metadata": {
        "id": "HHMyKKQ9oIqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like our training directory contains 8 subdirectories. Judging by their names, each contains the images for one of the classes in our dataset.\n",
        "\n",
        "Now that we know how our data is organized, let's check the distribution of our classes. In order to do this we'll need to count the number of files in each subdirectory. We'll store our results in a pandas Series() for easy data visualization."
      ],
      "metadata": {
        "id": "RjBwcmxMo9G0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.13"
      ],
      "metadata": {
        "id": "_aqS1Xkco_5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the for loop so that class_distributions_dict contains the name of each subdirectory as its keys and the number of files in each subdirectory as its values."
      ],
      "metadata": {
        "id": "HD6omP0lpjIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_distributions_dict = {}\n",
        "\n",
        "for subdirectory in class_directories:\n",
        "    dir = os.path.join(train_dir, subdirectory)\n",
        "    files = os.listdir(dir)\n",
        "    num_files = len(files)\n",
        "    class_distributions_dict[subdirectory] = num_files\n",
        "\n",
        "# converts the classes to Pandas series\n",
        "class_distributions = pd.Series(class_distributions_dict)\n",
        "\n",
        "print(\"class_distributions type:\", type(class_distributions))\n",
        "print(\"class_distributions shape:\", class_distributions.shape)\n",
        "print(class_distributions)\n",
        "\n",
        "# output\n",
        "class_distributions type: <class 'pandas.core.series.Series'>\n",
        "class_distributions shape: (8,)\n",
        "hog                  978\n",
        "blank               2213\n",
        "monkey_prosimian    2492\n",
        "antelope_duiker     2474\n",
        "leopard             2254\n",
        "civet_genet         2423\n",
        "bird                1641\n",
        "rodent              2013\n",
        "dtype: int64"
      ],
      "metadata": {
        "id": "kTrMDwTuqCWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.14: Create a bar chart from class_distributions"
      ],
      "metadata": {
        "id": "q9UVF1IusPz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar plot of class distributions\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Plot the data\n",
        "ax.bar(class_distributions.index, class_distributions.values)  # Write your code here\n",
        "ax.set_xlabel(\"Class Label\")\n",
        "ax.set_ylabel(\"Frequency [count]\")\n",
        "ax.set_title(\"Class Distribution, Multiclass Training Set\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# This is create a bar chart for the class_distributions"
      ],
      "metadata": {
        "id": "YLviihYRsc_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Images"
      ],
      "metadata": {
        "id": "mAc-HRj2w8LH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know the distribution of our data, but what do the actual images look like? Let's select a couple to explore further. Here are the paths for a hog and an antelope. üê∑ü¶å"
      ],
      "metadata": {
        "id": "CE5jZanyw_ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.15"
      ],
      "metadata": {
        "id": "RJGfaFWUxFj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path for hog image\n",
        "hog_image_path = os.path.join(train_dir, \"hog\", \"ZJ000072.jpg\")\n",
        "\n",
        "# Define path for antelope image\n",
        "antelope_image_path = os.path.join(train_dir, \"antelope_duiker\", \"ZJ002533.jpg\")\n",
        "\n",
        "print(\"hog_image_path type:\", type(hog_image_path))\n",
        "print(hog_image_path)\n",
        "print()\n",
        "print(\"antelope_image_path type:\", type(antelope_image_path))\n",
        "print(antelope_image_path)\n",
        "\n",
        "# output\n",
        "hog_image_path type: <class 'str'>\n",
        "data_p1/data_multiclass/train/hog/ZJ000072.jpg\n",
        "\n",
        "antelope_image_path type: <class 'str'>\n",
        "data_p1/data_multiclass/train/antelope_duiker/ZJ002533.jpg\n"
      ],
      "metadata": {
        "id": "aaV-3hhHxQEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load these images, we'll use the Pillow library (aka PIL), which comes with lots of tools for image processing. We'll start with the hog."
      ],
      "metadata": {
        "id": "jgswURhtyGOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hog_image_pil = Image.open(hog_image_path)\n",
        "\n",
        "print(\"hog_image_pil type:\", type(hog_image_pil))\n",
        "hog_image_pil\n",
        "\n",
        "# output\n",
        "hog_image_pil type: <class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
        "# shows the image of the hog in (Black and White)"
      ],
      "metadata": {
        "id": "9qRxDoJux8-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next up, the antelope."
      ],
      "metadata": {
        "id": "UD4jddhoyVzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "antelope_image_pil = Image.open(antelope_image_path)\n",
        "\n",
        "print(\"antelope_image_pil type:\", type(antelope_image_pil))\n",
        "antelope_image_pil\n",
        "\n",
        "# shows the image of the Antelope in (Coloured)"
      ],
      "metadata": {
        "id": "g1M8DEeryWbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.16 Image Attributes (Size and Mode)"
      ],
      "metadata": {
        "id": "b6-aaAjF0Ui_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Size - Tells us what the pixel is in terms of width and height.\n",
        "Mode - Tells us wheter it is coloured or black and white image"
      ],
      "metadata": {
        "id": "qoKyT2DO0rI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get image size\n",
        "hog_image_pil_size = hog_image_pil.size\n",
        "\n",
        "# Get image mode\n",
        "hog_image_pil_mode = hog_image_pil.mode\n",
        "\n",
        "# Print results\n",
        "print(\"hog_image_pil_size class:\", type(hog_image_pil_size))\n",
        "print(\"hog_image_pil_size length:\", len(hog_image_pil_size))\n",
        "print(\"Hog image size:\", hog_image_pil_size)\n",
        "print()\n",
        "print(\"hog_image_pil_mode class:\", type(hog_image_pil_mode))\n",
        "print(\"Hog image mode:\", hog_image_pil_mode)\n",
        "\n",
        "# output\n",
        "hog_image_pil_size class: <class 'tuple'>\n",
        "hog_image_pil_size length: 2\n",
        "Hog image size: (640, 360)\n",
        "\n",
        "hog_image_pil_mode class: <class 'str'>\n",
        "Hog image mode: L"
      ],
      "metadata": {
        "id": "eTvbRbbQ0brm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the antelope"
      ],
      "metadata": {
        "id": "SdqixlMj2Dew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get image size\n",
        "antelope_image_pil_size = antelope_image_pil.size\n",
        "\n",
        "# Get image mode\n",
        "antelope_image_pil_mode = antelope_image_pil.mode\n",
        "\n",
        "# Get image mode\n",
        "print(\"antelope_image_pil_size class:\", type(antelope_image_pil_size))\n",
        "print(\"antelope_image_pil_size length:\", len(antelope_image_pil_size))\n",
        "print(\"Antelope image size:\", antelope_image_pil_size)\n",
        "print()\n",
        "print(\"antelope_image_pil_mode class:\", type(antelope_image_pil_mode))\n",
        "print(\"Antelope image mode:\", antelope_image_pil_mode)\n",
        "\n",
        "# output\n",
        "antelope_image_pil_size class: <class 'tuple'>\n",
        "antelope_image_pil_size length: 2\n",
        "Antelope image size: (960, 540)\n",
        "\n",
        "antelope_image_pil_mode class: <class 'str'>\n",
        "Antelope image mode: RGB"
      ],
      "metadata": {
        "id": "y8E54fY74PuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the images mode, Hog is L meaning Black and White, while Antelope is RGB meaning coloured"
      ],
      "metadata": {
        "id": "nwxawFwB2Oq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also from their sizes, the Antelope is big than the Hog"
      ],
      "metadata": {
        "id": "pBT4zSnP2d_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These differences are important because all the images in our dataset must have the same size and mode before we can use them to train a model."
      ],
      "metadata": {
        "id": "-pPJwq7F2-Uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Tensor"
      ],
      "metadata": {
        "id": "ykDEDeMa3ns5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've loaded two image files using the Pillow library. For that reason, they're represented using the JpegImageFile() class. However, we'll need to represent them as tensors if we want to train a model."
      ],
      "metadata": {
        "id": "ONqpU88X4gXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PyTorch community has created the torchvision library, which comes with lots of helpful transformation tools. We can use the ToTensor() class to convert hog_image_pil to a tensor."
      ],
      "metadata": {
        "id": "uCH0jcvb4jbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hog_tensor = transforms.ToTensor()(hog_image_pil)\n",
        "\n",
        "print(\"hog_tensor type:\", type(hog_tensor))\n",
        "print(\"hog_tensor shape:\", hog_tensor.shape)\n",
        "print(\"hog_tensor dtype:\", hog_tensor.dtype)\n",
        "print(\"hog_tensor device:\", hog_tensor.device)\n",
        "\n",
        "# output\n",
        "hog_tensor type: <class 'torch.Tensor'>\n",
        "hog_tensor shape: torch.Size([1, 360, 640])\n",
        "hog_tensor dtype: torch.float32\n",
        "hog_tensor device: cpu"
      ],
      "metadata": {
        "id": "GaHtrASK2GBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Take a moment to examine the syntax we used to convert the hog image into a tensor.üîç ToTensor() is a class. (You can check out the class definition here.) However, we're using it like a function, combining it with another set of parenthesis that contains hog_image_pill as if it was an argument.\n",
        "The reason this works is that the ToTensor() class definition includes a __call__ method. This allows us to use the class like a function. Keep this in mind for the next lesson, where we'll create our own class for transforming images. ü§ì*"
      ],
      "metadata": {
        "id": "sCt4WgID3rHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "next, is to convert the antelope image to tensor"
      ],
      "metadata": {
        "id": "kpr6UQdN5zZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "antelope_tensor = transforms.ToTensor()(antelope_image_pil)\n",
        "\n",
        "print(\"antelope_tensor type:\", type(antelope_tensor))\n",
        "print(\"antelope_tensor shape:\", antelope_tensor.shape)\n",
        "print(\"antelope_tensor dtype:\", antelope_tensor.dtype)\n",
        "print(\"antelope_tensor device:\", antelope_tensor.device)\n",
        "\n",
        "# output\n",
        "antelope_tensor type: <class 'torch.Tensor'>\n",
        "antelope_tensor shape: torch.Size([3, 540, 960])\n",
        "antelope_tensor dtype: torch.float32\n",
        "antelope_tensor device: cpu"
      ],
      "metadata": {
        "id": "Qpz_-zvB9vwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Looking at the shape of these two tensors, we can see that they're both 3-dimensional. We can also see that some of the dimensions correspond to image height and width. For example, the shape of hog_tensor is [1, 360, 640]. The image's height is 360 pixels, and it's width is 640 pixels. But what does the first dimension correspond to? What does the 1 mean?*\n"
      ],
      "metadata": {
        "id": "sDqfUa7n_z0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*In addition to height and width, image files generally come with color channels. A color channel holds information about the intensity of a specific color for each pixel in an image. Because our hog image is grayscale, there's only one color to represent: gray. In fact, if we extract the values from the gray channel in hog_tensor and plot them, we end up with the same image we saw in the last section.*\n",
        "\n"
      ],
      "metadata": {
        "id": "tBHCiS1C_5L6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RGB Channel Plotting"
      ],
      "metadata": {
        "id": "7r3CUGYSAQ0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create figure with single axis\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "\n",
        "# Plot gray channel of hog_tensor\n",
        "ax.imshow(hog_tensor[0, :, :])\n",
        "\n",
        "# Turn off x- and y-axis\n",
        "ax.axis(\"off\")\n",
        "\n",
        "# Set title\n",
        "ax.set_title(\"Hog, grayscale\");\n",
        "\n",
        "# output\n",
        "# This output the hog image with with a form of one color shade"
      ],
      "metadata": {
        "id": "ICoellEZAFBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the hog image is grayscale, the antelope image is in color. Its mode is RGB, which stands red, green, and blue. Each of these colors has its own channel in the image. That's where the 3 in the antelope_tensor shape [3, 540, 960] comes from. We can extract the values for each channel using our slicing skills and plot them side-by-side."
      ],
      "metadata": {
        "id": "4JugLfMMEZJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.18: Complete the code below to plot the red, green, and blue channels of antelope_tensor."
      ],
      "metadata": {
        "id": "RTQJZU_IEaX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create figure with 3 subplots\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot red channel\n",
        "red_channel = antelope_tensor[0, :, :]\n",
        "ax0.imshow(red_channel, cmap=\"Reds\")\n",
        "ax0.set_title(\"Antelope, Red Channel\")\n",
        "ax0.axis(\"off\")\n",
        "\n",
        "# Plot green channel\n",
        "green_channel = antelope_tensor[1, :, :]\n",
        "ax1.imshow(green_channel, cmap=\"Greens\")\n",
        "ax1.set_title(\"Antelope, Green Channel\")\n",
        "ax1.axis(\"off\")\n",
        "\n",
        "# Plot blue channel\n",
        "blue_channel = antelope_tensor[2, :, :]\n",
        "ax2.imshow(blue_channel, cmap=\"Blues\")\n",
        "ax2.set_title(\"Antelope, Blue Channel\")\n",
        "ax2.axis(\"off\")\n",
        "\n",
        "plt.tight_layout();\n",
        "\n",
        "#output\n",
        "# 3 images representing each image channel red, green, blue with coloures as such"
      ],
      "metadata": {
        "id": "ngvuKfVEEUFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key takeaway is that the dimensions for an image tensor are always (C x H x W), channel by height by width."
      ],
      "metadata": {
        "id": "eleuHDxZEh3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know how the values in an image tensor are organized, but we haven't looked at the values themselves. Focusing on the antelope_tensor only, let's check its minimum and maximum values using the .amax() and .amin() methods."
      ],
      "metadata": {
        "id": "ZPkcbjZFFIYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.19"
      ],
      "metadata": {
        "id": "IiPN_CNBFbij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the minimum and maximum values of antelope_tensor and assign the results to max_channel_values and min_channel_values, respectively."
      ],
      "metadata": {
        "id": "CLdIJhq5FWwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_channel_values = antelope_tensor.amax()\n",
        "min_channel_values = antelope_tensor.amin()\n",
        "\n",
        "print(\"max_channel_values class:\", type(max_channel_values))\n",
        "print(\"max_channel_values shape:\", max_channel_values.shape)\n",
        "print(\"max_channel_values data type:\", max_channel_values.dtype)\n",
        "print(\"max_channel_values device:\", max_channel_values.device)\n",
        "print(\"Max values in antelope_tensor:\", max_channel_values)\n",
        "print()\n",
        "print(\"min_channel_values class:\", type(min_channel_values))\n",
        "print(\"min_channel_values shape:\", min_channel_values.shape)\n",
        "print(\"min_channel_values data type:\", min_channel_values.dtype)\n",
        "print(\"min_channel_values device:\", min_channel_values.device)\n",
        "print(\"Min values in antelope_tensor:\", min_channel_values)\n",
        "\n",
        "#output\n",
        "max_channel_values class: <class 'torch.Tensor'>\n",
        "max_channel_values shape: torch.Size([])\n",
        "max_channel_values data type: torch.float32\n",
        "max_channel_values device: cpu\n",
        "Max values in antelope_tensor: tensor(1.)\n",
        "\n",
        "min_channel_values class: <class 'torch.Tensor'>\n",
        "min_channel_values shape: torch.Size([])\n",
        "min_channel_values data type: torch.float32\n",
        "min_channel_values device: cpu\n",
        "Min values in antelope_tensor: tensor(0.)"
      ],
      "metadata": {
        "id": "8UlcD9ROFG39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the values in the tensor range from 0 to 1. 0 means that the color intensity at a particular pixel is 0%; 1 means intensity is 100%."
      ],
      "metadata": {
        "id": "USKy-pwcG_G0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*It's equally common to see the values in an image tensor range from 0 to 255. In fact, that's how the values in our image files are actually stored. However, the ToTensor() class automatically converts PIL images from [0, 255] to [0, 1]. So it's always a good idea to double-check image tensor values before building a model. ü§ì*"
      ],
      "metadata": {
        "id": "Zo3stMGHHANi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we'll do an aggregation calculation to find the mean value for each color channel in antelope_tensor. Remember that the color channel is the first dimension in the tensor (index position 0 in Python). This means we want to reduce along the other two dimensions, height and width. They are at index positions 1 and 2, respectively."
      ],
      "metadata": {
        "id": "XrmBsiAPHHq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1.1.20"
      ],
      "metadata": {
        "id": "PVoEI7dzVVkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the mean values of the separate color channels in antelope_tensor and assign the result to mean_channel_values."
      ],
      "metadata": {
        "id": "o4f92kWCVStf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_channel_values = torch.mean(antelope_tensor, dim=(1, 2))\n",
        "\n",
        "print(\"mean_channel_values class:\", type(mean_channel_values))\n",
        "print(\"mean_channel_values shape:\", mean_channel_values.shape)\n",
        "print(\"mean_channel_values dtype:\", mean_channel_values.dtype)\n",
        "print(\"mean_channel_values device:\", mean_channel_values.device)\n",
        "print(\"Mean channel values in antelope_tensor (RGB):\", mean_channel_values)\n",
        "\n",
        "# output\n",
        "mean_channel_values class: <class 'torch.Tensor'>\n",
        "mean_channel_values shape: torch.Size([3])\n",
        "mean_channel_values dtype: torch.float32\n",
        "mean_channel_values device: cpu\n",
        "Mean channel values in antelope_tensor (RGB): tensor([0.2652, 0.3679, 0.3393])"
      ],
      "metadata": {
        "id": "wsmzeml5VN5V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNwOMgGSDOx908ujhAmS1I/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}